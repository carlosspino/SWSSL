{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbFqBNCOgkKI+QkYM86n9Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosspino/SWSSL/blob/main/SWSSL_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AUMCPay0mrmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEURAL NETWORK PROJECT: SWSSL, Sliding window-based self-supervised learning for anomaly detection in high-resolution images **\n",
        "\n",
        "\n",
        "\n",
        "We extend anomaly detection to high-resolution images by proposing to train the network and perform inference at the patch level, through the sliding window algorithm. The model is trained on chest or DBT (Digital Breast Tomosynthesis) images, and the training process involves learning features that can discriminate between positive (normal) and negative (anomalous) samples.\n"
      ],
      "metadata": {
        "id": "c9TVLLCTms13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3r_YAP7dnusX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 5 methods in our train_network_dbt.py class which are:\n",
        "\n",
        "-twin_loss(f_patch1, f_patch2, f_neg=None, p=False, target=None)\n",
        "\n",
        "-train(model, device, args)\n",
        "\n",
        "\n",
        "-create_dataloader(dataset, batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "-evaluate_and_save_model(epoch, model, args, best_score)\n",
        "\n",
        "-get_args()"
      ],
      "metadata": {
        "id": "bj9a-7T2m2Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***twin_loss Method***\n",
        "\n",
        "**1-Unpack data:**\n",
        "\n",
        "  We have to understand the batch size and featuer dimension of the image patches. Necessary to calculate the loss and to ensure that comptations are perfomed well on batches of data.\n",
        "\n",
        "**2-Feature normalizations:**\n",
        "\n",
        "We use the Z-score, we subtract the mean and divide by the standard deviation of each feature. It helps the model to converge faster.\n",
        "\n",
        "**3-The positive score: **\n",
        "\n",
        "How to quantify how similar the patches are to each other.\n",
        "\n",
        "**4-Calculation of Difference and Loss:**\n",
        "\n",
        "We calculate the difference between the positive score and an identity matrix.\n",
        "The difference allows us to penalize any discrepancies between the elements of the positive score. Summing the diagonal of this difference is giving us an initial loss, represents the discrepancy between the features of patches that we aim to minimize during training.\n",
        "\n",
        "**5-Weight for Non-diagonal elements:**\n",
        "\n",
        "We get the non-diagonal elements of the difference to penalize discrepancies between the non-diagonal elements of the positive score.\n",
        "Then, we multiply the difference by this weight and sum it to the initial loss to get the total loss.\n",
        "\n",
        "**6-Additional Loss Calculation:**\n",
        "\n",
        "Distinguish between pairs of features that shoulld be similar and those that should be different."
      ],
      "metadata": {
        "id": "gHBFgnAArT3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def twin_loss(f_patch1, f_patch2, f_neg=None, p=False, target=None):\n",
        "    batch_size, dimension = f_patch1.shape\n",
        "\n",
        "    # Features normalizations\n",
        "    f_patch1_norm = F.normalize(f_patch1, dim=-1)\n",
        "    f_patch2_norm = F.normalize(f_patch2, dim=-1)\n",
        "\n",
        "    # Calculation of positive loss\n",
        "    pos_score = torch.mm(f_patch1_norm, f_patch2_norm.t()) / batch_size\n",
        "    diff = (pos_score - torch.eye(batch_size).cuda()).pow(2)\n",
        "    loss = diff.diag().sum()\n",
        "\n",
        "    # Non-diagonal loss weighting\n",
        "    non_diag_weight = (torch.ones([batch_size, batch_size]) - torch.eye(batch_size)) * 1e-6\n",
        "    non_diag_weight = non_diag_weight.cuda()\n",
        "    diff *= non_diag_weight\n",
        "    loss += diff.sum()\n",
        "\n",
        "    if f_neg is not None:\n",
        "        # Negative features normalization\n",
        "        f_neg_norm = F.normalize(f_neg, dim=-1)\n",
        "\n",
        "        # Loss calculation for positive and negative pairs\n",
        "        pair_score = torch.mm(f_patch1_norm, f_patch2_norm.t())\n",
        "        pair_sim = torch.sigmoid(pair_score.diag())\n",
        "        pair_loss = torch.abs(pair_sim - torch.ones(batch_size).cuda()).sum()\n",
        "\n",
        "        neg_score = torch.mm(f_patch1_norm, f_neg_norm.t())\n",
        "        neg_sim = torch.sigmoid(neg_score.diag())\n",
        "        neg_loss = torch.abs(neg_sim - target).sum()\n",
        "\n",
        "        # Loss sum\n",
        "        loss += neg_loss + pair_loss\n",
        "\n",
        "    # Some prints for debugging and have some tracking info\n",
        "    if p:\n",
        "        if f_neg is not None:\n",
        "            print('pair loss ', pair_loss.item())\n",
        "            print('neighbor loss ', neg_loss.item())\n",
        "        print('total loss ', loss.item())\n",
        "        print('feature sample:')\n",
        "        print(f_patch1_norm[0][:10])\n",
        "        print(f_patch2_norm[0][:10])\n",
        "        print(f_patch1_norm[1][:10])\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "aShHxFKbqtbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***train Method***, *3 aux methods explained too*\n",
        "\n",
        "**1-create_dataset:**\n",
        "\n",
        "We create dataset depending on its category (chest or dbt) and its phase (train or val). Uses the \"ChestDataset\" and \"DBTDataset\" classes to create datasets.\n",
        "\n",
        "**2-create_dataloader:**\n",
        "\n",
        "Create dataloaders for the dataset created in the previous method. Sets up the dataloaders with the batch size specified and other options like shuffling and dropping last incomplete batch.\n",
        "\n",
        "**3-evaluate_and_save_model:**\n",
        "\n",
        "Evaluates the model's performance and saves the model checkpoint if the current score is better than the best score seen so far. It calls the 'evaluate_image' method to evaluate the model on the train and test datasets.\n",
        "\n",
        "**4-Dataloader setup:**\n",
        "\n",
        "These dataloaders will be used during the training loop to iterate over batches of data.\n",
        "\n",
        "**5-Optimizer Setup:**\n",
        "\n",
        "Uses a stochastic gradient descent (SGD) with momentum and weight decay. It will be used to update the model's parameters based on the calculated gradients during training.\n",
        "\n",
        "**6-Training Loop:**\n",
        "\n",
        "It performs backpropagation to update the model's parameters based on the calculated loss.\n",
        "\n",
        "**7-Model evaluation and saving:**\n",
        "\n",
        "Is evaluated every 10 epochs using the 'evaluate_and_save_model' method. If the model's performance improves, the model checkpoint is saved.\n"
      ],
      "metadata": {
        "id": "522vhF-8y881"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, args):\n",
        "\n",
        "    def create_dataset(category, phase, patch=True):\n",
        "        transforms_list = [\n",
        "            transforms.Resize((256*4, 256*4), Image.ANTIALIAS),\n",
        "            transforms.ToTensor()\n",
        "        ] if patch else [\n",
        "            transforms.Resize((256*4, 256*3), Image.ANTIALIAS),\n",
        "        ]\n",
        "\n",
        "        transforms_list = transforms.Compose(transforms_list)\n",
        "\n",
        "        if category == 'chest':\n",
        "            return ChestDataset(\n",
        "               root=args.dataset_path,\n",
        "                pre_transform=transforms_list,\n",
        "            phase=phase,\n",
        "                patch=patch,\n",
        "                patch_size=args.patch_size,\n",
        "                step_size=args.step_size\n",
        "            )\n",
        "        elif category == 'dbt':\n",
        "            return DBTDataset(\n",
        "                root=args.dataset_path,\n",
        "                pre_transform=transforms_list,\n",
        "                phase=phase,\n",
        "                patch=patch,\n",
        "                patch_size=args.patch_size,\n",
        "                step_size=args.step_size\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid category: {category}\")\n",
        "\n",
        "    def create_dataloader(dataset, batch_size, shuffle=True, drop_last=False):\n",
        "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    def evaluate_and_save_model(epoch, model, args, best_score):\n",
        "        twin_loss(f_patch, f_aug, f_neg=f_patch2, target=sim, p=1)\n",
        "        score = evaluate_image(args, model, train_loader, test_loader, device, category=args.category)\n",
        "        if score > best_score:\n",
        "            torch.save(model.state_dict(), f'checkpoints/{args.category}_{epoch}_{score}.pth')\n",
        "            best_score = score\n",
        "        print(f'img lv curr acc {score}, best acc {best_score}')\n",
        "\n",
        "\n",
        "    # Dataloader\n",
        "    train_patch_d = create_dataset(args.category, 'train', patch=True)\n",
        "    train_full_d = create_dataset(args.category, 'train', patch=False)\n",
        "    test_full_d = create_dataset(args.category, 'val', patch=False)\n",
        "\n",
        "    train_patch_loader = create_dataloader(train_patch_d, args.batch_size, shuffle=True)\n",
        "    train_loader = create_dataloader(train_full_d, args.batch_size, shuffle=False, drop_last=False)\n",
        "    test_loader = create_dataloader(test_full_d, 1, shuffle=False)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=1e-5)\n",
        "\n",
        "    best_score = -1\n",
        "    score = evaluate_image(args, model, train_loader, test_loader, device, category=args.category)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        with tqdm(total=len(train_patch_d), desc=f'Epoch {epoch + 1} / {args.epochs}', unit='img') as pbar:\n",
        "            for idx, data in enumerate(train_patch_loader):\n",
        "                img, img_aug, img_2, sim = data\n",
        "\n",
        "                img = img.to(device)\n",
        "                img_2 = img_2.to(device)\n",
        "                img_aug = img_aug.to(device)\n",
        "                sim = sim.to(device)\n",
        "\n",
        "                f_patch, tmp = model(img)\n",
        "                f_patch2, _ = model(img_2)\n",
        "                f_aug, _ = model(img_aug)\n",
        "\n",
        "                loss = twin_loss(f_patch, f_aug, f_neg=f_patch2, target=sim)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
        "                optimizer.step()\n",
        "\n",
        "                # tqdm Update\n",
        "                pbar.set_postfix(**{'twin loss': loss.item()})\n",
        "                pbar.update(img.shape[0])\n",
        "\n",
        "        # Evaluate\n",
        "        if epoch > 0 and epoch % 10 == 0:\n",
        "            evaluate_and_save_model(epoch, model, args, best_score)"
      ],
      "metadata": {
        "id": "XJdYE9wfzD7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***def_args() Method:***\n",
        "\n",
        "**1-** This method uses 'argparse' module to analyze the arguments of the command line. Define a parser and add some arguments that can be given through the command line to the script. Some of these arguments are the training phase, the path of the dataset, batchsize, hyperparameters etc.\n",
        "\n",
        "**2-** We initialize the execution device (GPU, if not, CPU), and obtains the command line arguments using the 'get_args()' method.\n",
        "\n",
        "The model is created, and training is initiated by calling the 'train' function\n"
      ],
      "metadata": {
        "id": "hIgLX4szzaag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='ANOMALYDETECTION')\n",
        "\n",
        "    # General settings\n",
        "    parser.add_argument('--phase', choices=['train', 'test'], default='train')\n",
        "    parser.add_argument('--dataset_path', default='../dbt_dataset')\n",
        "    parser.add_argument('--category', default='dbt')\n",
        "    parser.add_argument('--batch_size', type=int, default=300)\n",
        "    parser.add_argument('--load_size', default=256)  # 256\n",
        "    parser.add_argument('--input_size', default=256)\n",
        "    parser.add_argument('--coreset_sampling_ratio', default=0.01)\n",
        "    parser.add_argument('--project_root_path', default='results')\n",
        "    parser.add_argument('--save_src_code', default=True)\n",
        "    parser.add_argument('--save_anomaly_map', default=True)\n",
        "\n",
        "    # Model hyperparameters\n",
        "    parser.add_argument('--n_neighbors', type=int, default=9)\n",
        "    parser.add_argument('--lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--k', type=int, default=9)\n",
        "    parser.add_argument('--learning-rate-weights', default=0.01, type=float, metavar='LR',\n",
        "                        help='base learning rate for weights')\n",
        "    parser.add_argument('--learning-rate-biases', default=0.0048, type=float, metavar='LR',\n",
        "                        help='base learning rate for biases and batch norm parameters')\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-6)\n",
        "\n",
        "    # Training settings\n",
        "    parser.add_argument('--epochs', type=int, default=10000)\n",
        "    parser.add_argument('--patch_size', type=int, default=128)\n",
        "    parser.add_argument('--step_size', type=int, default=32)\n",
        "    parser.add_argument('--use_tumor', type=int, default=0)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args = get_args()\n",
        "\n",
        "    model = Patch_Model(input_channel=3)\n",
        "    model.to(device)\n",
        "    train(model, device, args)"
      ],
      "metadata": {
        "id": "40AS7LBf22a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To sum up, this script provides a framework for training anomaly detection models on medical image datasets using PyTorch. We separated functions for dataset creation, data loading, model evaluation and training loop; providing a modular implementation.\n",
        "We think this is a comprehensive pipeline for training an anomaly detection.\n",
        "\n",
        "\n",
        "Carlos Pino Padilla and Carlos Ramírez Rodríguez de Sepúlveda.\n",
        "Neural Networks, Università la Sapienza."
      ],
      "metadata": {
        "id": "Q3-o49G23Nbo"
      }
    }
  ]
}